{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchao\n",
        "!pip install git+https://github.com/huggingface/diffusers"
      ],
      "metadata": {
        "id": "RnlMOqAly13U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88hRw0aQwesR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import AutoencoderKLCogVideoX, CogVideoXTransformer3DModel, CogVideoXImageToVideoPipeline\n",
        "from diffusers.utils import export_to_video, load_image\n",
        "from transformers import T5EncoderModel\n",
        "from torchao.quantization import quantize_, int8_weight_only\n",
        "\n",
        "quantization = int8_weight_only\n",
        "\n",
        "text_encoder = T5EncoderModel.from_pretrained(\"THUDM/CogVideoX1.5-5B-I2V\", subfolder=\"text_encoder\",\n",
        "                                              torch_dtype=torch.bfloat16)\n",
        "quantize_(text_encoder, quantization())\n",
        "\n",
        "transformer = CogVideoXTransformer3DModel.from_pretrained(\"THUDM/CogVideoX1.5-5B-I2V\", subfolder=\"transformer\",\n",
        "                                                          torch_dtype=torch.bfloat16)\n",
        "quantize_(transformer, quantization())\n",
        "\n",
        "vae = AutoencoderKLCogVideoX.from_pretrained(\"THUDM/CogVideoX1.5-5B-I2V\", subfolder=\"vae\", torch_dtype=torch.bfloat16)\n",
        "quantize_(vae, quantization())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import imageio\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Define parameters for the pipeline\n",
        "NUM_FRAMES = 16\n",
        "NUM_INF_STEPS = 50\n",
        "NUM_VIDEOS = 1\n",
        "GUIDANCE = 6\n",
        "HEIGHT, WIDTH = 256, 256\n",
        "mean = 0  # Mean of the Gaussian noise\n",
        "std_dev = 25  # Standard deviation of the Gaussian noise\n",
        "\n",
        "\n",
        "def process_frame_and_generate_video(first_frame_pil, output_video_path, prompt):\n",
        "\n",
        "    image = np.array(first_frame_pil).astype(np.float16) #use pillow image\n",
        "\n",
        "    print(image.shape)\n",
        "\n",
        "    noise = np.random.normal(mean, std_dev, image.shape)\n",
        "\n",
        "    noisy_image = image + noise\n",
        "\n",
        "    noisy_image = np.clip(noisy_image, 0, 255).astype(np.uint8)\n",
        "\n",
        "    images = [noisy_image for _ in range(NUM_FRAMES)]\n",
        "\n",
        "    images = np.stack(images, axis=0)\n",
        "\n",
        "    print(images.shape)\n",
        "\n",
        "    # Convert the image to tensor and move it to the correct device\n",
        "    images = torch.from_numpy(images).to('cuda', dtype=torch.bfloat16)\n",
        "\n",
        "    reshaped_image = images.reshape(1, 3, NUM_FRAMES, HEIGHT, WIDTH)\n",
        "\n",
        "\n",
        "    # print(reshaped_image.shape)\n",
        "    # Pass through the VAE to obtain latents\n",
        "    latents = pipe.vae.encode(reshaped_image).latent_dist\n",
        "\n",
        "    alpha = 1/100000 # adjust alpha\n",
        "    latents=alpha*latents.mean.reshape(1, NUM_FRAMES // 4, 16, HEIGHT // 8, WIDTH // 8)\n",
        "\n",
        "    generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
        "\n",
        "    beta = 1 # adjust beta\n",
        "    random_tensor = beta*torch.randn((1, NUM_FRAMES // 4, 16, HEIGHT // 8, WIDTH // 8), generator=generator, device=\"cuda\").to(dtype=torch.bfloat16)\n",
        "\n",
        "    latents = latents + random_tensor\n",
        "\n",
        "    print(latents.shape)\n",
        "    video = pipe(\n",
        "        prompt=prompt,\n",
        "        image=first_frame_pil,\n",
        "        num_videos_per_prompt=NUM_VIDEOS,\n",
        "        num_inference_steps=NUM_INF_STEPS,\n",
        "        num_frames=NUM_FRAMES,\n",
        "        guidance_scale=GUIDANCE,\n",
        "        latents=latents,\n",
        "        generator=torch.Generator(device=\"cuda\").manual_seed(42),\n",
        "        height=HEIGHT,\n",
        "        width=WIDTH\n",
        "    ).frames[0]\n",
        "\n",
        "    init_path = output_video_path.split('.')[0] + '_init.mp4'\n",
        "    export_to_video(video, init_path, fps=8)\n",
        "\n",
        "    # export_to_video(video, f\"outputvideo_init.mp4\", fps=8)\n",
        "\n",
        "    for i in range(NUM_FRAMES-1):\n",
        "\n",
        "        previous_images = images.to(dtype=torch.float16).cpu().numpy()\n",
        "\n",
        "        last_frame = np.array(video[-1]).reshape(1,256,256,3)\n",
        "\n",
        "        del video\n",
        "\n",
        "        noise = np.random.normal(mean, std_dev, last_frame.shape)\n",
        "\n",
        "        noisy_image = last_frame + noise\n",
        "\n",
        "        noisy_image = np.clip(noisy_image, 0, 255).astype(np.uint8)\n",
        "\n",
        "        images = np.concatenate([previous_images[1:], noisy_image], axis=0)\n",
        "\n",
        "        images = torch.from_numpy(images).to('cuda', dtype=torch.bfloat16)\n",
        "\n",
        "        reshaped_image = images.reshape(1, 3, NUM_FRAMES, HEIGHT, WIDTH)\n",
        "\n",
        "        latents = pipe.vae.encode(reshaped_image).latent_dist\n",
        "\n",
        "        latents = 1/100000 * latents.mean.reshape(1, NUM_FRAMES // 4, 16, HEIGHT // 8, WIDTH // 8)\n",
        "\n",
        "        # image = load_image(image=\"afantou_d.jpg\")\n",
        "\n",
        "        generator = torch.Generator(device=\"cuda\").manual_seed(42)\n",
        "\n",
        "        random_tensor = torch.randn((1, NUM_FRAMES // 4, 16, HEIGHT // 8, WIDTH // 8), generator=generator, device=\"cuda\").to(dtype=torch.bfloat16)\n",
        "\n",
        "        latents = latents + random_tensor\n",
        "\n",
        "        video = pipe(\n",
        "            prompt=prompt,\n",
        "            image=first_frame_pil,\n",
        "            num_videos_per_prompt=NUM_VIDEOS,\n",
        "            num_inference_steps=NUM_INF_STEPS,\n",
        "            num_frames=NUM_FRAMES,\n",
        "            guidance_scale=GUIDANCE,\n",
        "            latents=latents,\n",
        "            generator=torch.Generator(device=\"cuda\").manual_seed(42),\n",
        "            height=HEIGHT,\n",
        "            width=WIDTH\n",
        "        ).frames[0]\n",
        "\n",
        "    export_to_video(video, output_video_path, fps=8)\n"
      ],
      "metadata": {
        "id": "SGjhqi3swhxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UCF-101"
      ],
      "metadata": {
        "id": "VswOHo8nzAxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) 2024 Mitsubishi Electric Research Laboratories (MERL)\n",
        "# Copyright (c) 2021-2022 The Alibaba Fundamental Vision Team Authors. All rights reserved.\n",
        "#\n",
        "# SPDX-License-Identifier: AGPL-3.0-or-later\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "#\n",
        "# Code adapted from https://github.com/modelscope/modelscope/blob/57791a8cc59ccf9eda8b94a9a9512d9e3029c00b/modelscope/models/cv/anydoor/ldm/util.py -- Apache-2.0 license\n",
        "\n",
        "import importlib\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from inspect import isfunction\n",
        "import cv2\n",
        "import imageio\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "\n",
        "\n",
        "def log_txt_as_img(wh, xc, size=10):\n",
        "    # wh a tuple of (width, height)\n",
        "    # xc a list of captions to plot\n",
        "    b = len(xc)\n",
        "    txts = list()\n",
        "    for bi in range(b):\n",
        "        txt = Image.new(\"RGB\", wh, color=\"white\")\n",
        "        draw = ImageDraw.Draw(txt)\n",
        "        font = ImageFont.truetype(\"data/DejaVuSans.ttf\", size=size)\n",
        "        nc = int(40 * (wh[0] / 256))\n",
        "        lines = \"\\n\".join(xc[bi][start : start + nc] for start in range(0, len(xc[bi]), nc))\n",
        "\n",
        "        try:\n",
        "            draw.text((0, 0), lines, fill=\"black\", font=font)\n",
        "        except UnicodeEncodeError:\n",
        "            print(\"Cant encode string for logging. Skipping.\")\n",
        "\n",
        "        txt = np.array(txt).transpose(2, 0, 1) / 127.5 - 1.0\n",
        "        txts.append(txt)\n",
        "    txts = np.stack(txts)\n",
        "    txts = torch.tensor(txts)\n",
        "    return txts\n",
        "\n",
        "\n",
        "def ismap(x):\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "        return False\n",
        "    return (len(x.shape) == 4) and (x.shape[1] > 3)\n",
        "\n",
        "\n",
        "def isimage(x):\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "        return False\n",
        "    return (len(x.shape) == 4) and (x.shape[1] == 3 or x.shape[1] == 1)\n",
        "\n",
        "\n",
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "\n",
        "def mean_flat(tensor):\n",
        "    \"\"\"\n",
        "    https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/nn.py#L86\n",
        "    Take the mean over all non-batch dimensions.\n",
        "    \"\"\"\n",
        "    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n",
        "\n",
        "\n",
        "def count_params(model, verbose=False):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    if verbose:\n",
        "        print(f\"{model.__class__.__name__} has {total_params*1.e-6:.2f} M params.\")\n",
        "    return total_params\n",
        "\n",
        "\n",
        "def instantiate_from_config(config):\n",
        "    if not \"target\" in config:\n",
        "        if config == \"__is_first_stage__\":\n",
        "            return None\n",
        "        elif config == \"__is_unconditional__\":\n",
        "            return None\n",
        "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
        "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
        "\n",
        "\n",
        "def get_obj_from_str(string, reload=False):\n",
        "    module, cls = string.rsplit(\".\", 1)\n",
        "    if reload:\n",
        "        module_imp = importlib.import_module(module)\n",
        "        importlib.reload(module_imp)\n",
        "    return getattr(importlib.import_module(module, package=None), cls)\n",
        "\n",
        "\n",
        "def center_crop(img, new_width=None, new_height=None):\n",
        "    width = img.shape[1]\n",
        "    height = img.shape[0]\n",
        "\n",
        "    if width == height:\n",
        "        return img\n",
        "\n",
        "    if new_width is None:\n",
        "        new_width = min(width, height)\n",
        "\n",
        "    if new_height is None:\n",
        "        new_height = min(width, height)\n",
        "\n",
        "    left = int(np.ceil((width - new_width) / 2))\n",
        "    right = width - int(np.floor((width - new_width) / 2))\n",
        "\n",
        "    top = int(np.ceil((height - new_height) / 2))\n",
        "    bottom = height - int(np.floor((height - new_height) / 2))\n",
        "\n",
        "    if len(img.shape) == 2:\n",
        "        center_cropped_img = img[top:bottom, left:right]\n",
        "    else:\n",
        "        center_cropped_img = img[top:bottom, left:right, ...]\n",
        "\n",
        "    return center_cropped_img\n",
        "\n",
        "\n",
        "def setup_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def resize(img_npy, IMG_H, IMG_W):\n",
        "    return np.asarray(Image.fromarray(img_npy).resize((IMG_H, IMG_W)))\n",
        "\n",
        "\n",
        "def preprocess_image(img):\n",
        "    img_tensor = torch.from_numpy(img / 255.0).type(torch.float32)\n",
        "    img_tensor = img_tensor.unsqueeze(dim=0)\n",
        "    img_tensor = img_tensor.permute(0, 3, 1, 2)  # nchw\n",
        "    # normalization\n",
        "    mean = [0.5, 0.5, 0.5]\n",
        "    std = [0.5, 0.5, 0.5]\n",
        "    mean = torch.tensor(mean, device=img_tensor.device).reshape(1, -1, 1, 1)  # nchw\n",
        "    std = torch.tensor(std, device=img_tensor.device).reshape(1, -1, 1, 1)  # nchw\n",
        "    img_tensor = img_tensor.sub_(mean).div_(std)\n",
        "    return img_tensor\n",
        "\n",
        "\n",
        "def postprocess_image(img_tensor, batch_idx=0):\n",
        "    img_tensor = img_tensor.clone().detach().cpu()\n",
        "    mean = [0.5, 0.5, 0.5]\n",
        "    std = [0.5, 0.5, 0.5]\n",
        "    mean = torch.tensor(mean, device=img_tensor.device).reshape(1, -1, 1, 1)  # nchw\n",
        "    std = torch.tensor(std, device=img_tensor.device).reshape(1, -1, 1, 1)  # nchw\n",
        "    img_tensor = img_tensor.mul_(std).add_(mean)\n",
        "    img_tensor = img_tensor[batch_idx].permute(1, 2, 0)\n",
        "    img_tensor[img_tensor < 0] = 0\n",
        "    img_tensor[img_tensor > 1] = 1\n",
        "    img_data = np.array(img_tensor * 255, dtype=np.uint8)\n",
        "    return img_data\n",
        "\n",
        "\n",
        "def resize_with_border(im, desired_size, interpolation):\n",
        "    old_size = im.shape[:2]\n",
        "    ratio = float(desired_size) / max(old_size)\n",
        "    new_size = tuple(int(x * ratio) for x in old_size)\n",
        "\n",
        "    im = cv2.resize(im, (new_size[1], new_size[0]), interpolation=interpolation)\n",
        "    delta_w = desired_size - new_size[1]\n",
        "    delta_h = desired_size - new_size[0]\n",
        "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "\n",
        "    color = [0, 0, 0]\n",
        "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
        "\n",
        "    return new_im\n",
        "\n",
        "\n",
        "def nearest_true_index(mask, index):\n",
        "    if mask[index]:\n",
        "        return index  # The given index is True, so it's the nearest True index.\n",
        "\n",
        "    left_index = index - 1\n",
        "    right_index = index + 1\n",
        "\n",
        "    while left_index >= 0 or right_index < len(mask):\n",
        "        if left_index >= 0 and mask[left_index]:\n",
        "            return left_index\n",
        "        if right_index < len(mask) and mask[right_index]:\n",
        "            return right_index\n",
        "\n",
        "        left_index -= 1\n",
        "        right_index += 1\n",
        "\n",
        "    return None  # No True value found in the mask.\n",
        "\n",
        "\n",
        "def binary_to_hex(binary_list):\n",
        "    binary_list = deepcopy(binary_list)\n",
        "    # binary_list.reverse()  # Reverse the list to match bit order.\n",
        "    binary_string = \"\".join([str(int(bit)) for bit in binary_list])\n",
        "    decimal_number = int(binary_string, 2)\n",
        "    hex_string = hex(decimal_number).lstrip(\"0x\")\n",
        "    return hex_string\n",
        "\n",
        "\n",
        "def list2gif(img_path_list, gif_path, save_img_dir):\n",
        "    save_npy_list = [imageio.v2.imread(x) for x in img_path_list]\n",
        "    imageio.mimwrite(gif_path, save_npy_list, duration=1000 / 8)\n",
        "    for i, save_npy in enumerate(save_npy_list):\n",
        "        imageio.v2.imsave(os.path.join(save_img_dir, \"%04d.png\" % i), save_npy)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "\n",
        "# Function to center crop and resize a frame\n",
        "def process_frame(frame, size=256):\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "    cropped_frame = center_crop(frame_rgb)  # Center crop the frame\n",
        "    resized_frame = resize(cropped_frame, size, size)  # Resize to the desired size\n",
        "    resized_bgr = cv2.cvtColor(resized_frame, cv2.COLOR_RGB2BGR)  # Convert back to BGR\n",
        "    return resized_bgr\n",
        "\n",
        "# Function to process each video in the directory\n",
        "def process_video(input_path, output_path, output_size=256):\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video {input_path}\")\n",
        "        return\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    print(f\"Processing video: {input_path}\")\n",
        "    print(f\"FPS: {fps}, Total Frames: {total_frames}\")\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Codec for .mp4 format\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (output_size, output_size))\n",
        "\n",
        "    frame_count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break  # End of video\n",
        "\n",
        "        processed_frame = process_frame(frame, size=output_size)\n",
        "        out.write(processed_frame)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Optional: Progress tracking\n",
        "        if frame_count % 50 == 0:\n",
        "            print(f\"Processed {frame_count}/{total_frames} frames.\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"Video saved to: {output_path}\")\n",
        "\n",
        "# Directory containing the videos\n",
        "input_dir = \"videos\"  # Replace with your video directory\n",
        "\n",
        "# crop the videos in the desired shape (output_size)\n",
        "# Loop over all files in the directory\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith(\".avi\") or filename.endswith(\".mp4\"):  # Adjust for your video file extensions\n",
        "        input_video_path = os.path.join(input_dir, filename)\n",
        "        output_video_path = input_video_path[:-4] + \"proc.mp4\"\n",
        "\n",
        "        process_video(input_video_path, output_video_path, output_size=256)"
      ],
      "metadata": {
        "id": "nCFoeow_zT6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# get the num_frames first frames of the videos\n",
        "def extract_frames(video_path, output_dir, num_frames=32):\n",
        "    \"\"\"\n",
        "    Extracts the first 'num_frames' from a video and saves them in a folder.\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to the input video file.\n",
        "        output_dir: Path to the output directory.\n",
        "        num_frames: Number of frames to extract.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        os.makedirs(output_dir, exist_ok=True)  # Create output directory if it doesn't exist\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        frame_count = 0\n",
        "        while frame_count < num_frames and cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame_name = f\"frame_{frame_count:03d}.jpg\"  # Use zero-padding for consistent filenames\n",
        "            output_path = os.path.join(output_dir, frame_name)\n",
        "            cv2.imwrite(output_path, frame)\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "        cap.release()\n",
        "        print(f\"Extracted {frame_count} frames from {video_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {video_path}: {e}\")\n",
        "\n",
        "# Example usage\n",
        "input_dir = \"videos\"  # Replace with the actual directory\n",
        "output_dir_template = \"{video_name}\"\n",
        "\n",
        "for video_file in os.listdir(input_dir):\n",
        "    if video_file.endswith(\".mp4\"):\n",
        "        video_path = os.path.join(input_dir, video_file)\n",
        "        video_name = os.path.splitext(video_file)[0]  # Get video name without extension\n",
        "        output_dir = output_dir_template.format(video_name=video_name)\n",
        "        extract_frames(video_path, output_dir)"
      ],
      "metadata": {
        "id": "wjitx0h2zufA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video_directory(video_dir):\n",
        "    # Find the first frame image\n",
        "    frame_files = [f for f in os.listdir(video_dir) if f.endswith('.jpg')]\n",
        "    if not frame_files:\n",
        "        print(f\"No frames found in directory {video_dir}\")\n",
        "        return\n",
        "\n",
        "    first_frame_path = os.path.join(video_dir, \"frame_000.jpg\")\n",
        "\n",
        "    print(\"first file path\", first_frame_path)\n",
        "    first_frame = Image.open(first_frame_path)\n",
        "\n",
        "    width, height = first_frame.size\n",
        "    mode = first_frame.mode\n",
        "\n",
        "    print(f\"Width: {width} pixels\")\n",
        "    print(f\"Height: {height} pixels\")\n",
        "    print(f\"Mode: {mode}\")\n",
        "\n",
        "    if mode in (\"RGB\", \"RGBA\", \"CMYK\"):\n",
        "      num_channels = len(mode)\n",
        "      print(f\"Number of channels: {num_channels}\")\n",
        "    elif mode in (\"1\", \"L\", \"P\"):\n",
        "      num_channels = 1\n",
        "      print(f\"Number of channels: {num_channels}\")\n",
        "    else:\n",
        "        print(\"Number of channels could not be determined from the mode.\")\n",
        "\n",
        "    # Process the first frame and generate a new video\n",
        "    selected_class_dict = {\n",
        "        \"ApplyEyeMakeup\": \"A person is applying eye makeup.\",\n",
        "        \"BabyCrawling\": \"A Baby is crawling.\",\n",
        "        \"BreastStroke\": \"A person is performing breaststroke.\",\n",
        "        \"Drumming\": \"A person is drumming.\",\n",
        "        \"HorseRiding\": \"A person is riding horse.\",\n",
        "        \"Kayaking\": \"A person is kayaking.\",\n",
        "        \"PlayingGuitar\": \"A person is playing Guitar.\",\n",
        "        \"Surfing\": \"A person is surfing.\",\n",
        "        \"ShavingBeard\": \"A person is shaving beard.\",\n",
        "    }\n",
        "    prompt = selected_class_dict[video_dir.split('_')[1]]\n",
        "    print(prompt)\n",
        "    output_video_path = os.path.join(video_dir, \"generated_video.mp4\")\n",
        "    process_frame_and_generate_video(first_frame, output_video_path, prompt)\n",
        "\n",
        "def process_all_videos(root_dir):\n",
        "    # List all subdirectories that represent the processed video folders\n",
        "    video_dirs = [f for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]\n",
        "\n",
        "    if not video_dirs:\n",
        "        print(\"No video directories found.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(video_dirs)} video directories. Starting processing...\")\n",
        "\n",
        "    for video_dir in video_dirs:\n",
        "        video_dir_path = os.path.join(root_dir, video_dir)\n",
        "        if video_dir.startswith('.'):\n",
        "            continue\n",
        "        else :\n",
        "          print(\"video dir path\", video_dir_path)\n",
        "          pro = video_dir_path.split('_')[1]\n",
        "          print(pro)\n",
        "          process_video_directory(video_dir_path)\n",
        "\n",
        "    print(\"All videos processed successfully.\")\n",
        "\n",
        "\n",
        "# generate the videos\n",
        "root_dir = \"frms/\"  # Replace with the path where your video directories are stored\n",
        "process_all_videos(root_dir)"
      ],
      "metadata": {
        "id": "yUs0bZCpzCYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "WUEv7gGe3PfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code adapted from https://github.com/JunyaoHu/common_metrics_on_video_quality\n",
        "# download and save on the same directory the files of the repo\n",
        "import torch\n",
        "from calculate_fvd import calculate_fvd\n",
        "from calculate_psnr import calculate_psnr\n",
        "from calculate_ssim import calculate_ssim\n",
        "from calculate_lpips import calculate_lpips\n",
        "\n",
        "# ps: pixel value should be in [0, 1]!\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "def process_video(video_path, size=(256, 256)):\n",
        "    \"\"\"\n",
        "    Process a video file into a tensor of shape (VIDEO_LENGTH, CHANNEL, SIZE, SIZE).\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # Resize frame\n",
        "        frame = cv2.resize(frame, size)\n",
        "        # Convert BGR to RGB and append\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Convert to tensor (VIDEO_LENGTH, CHANNEL, SIZE, SIZE)\n",
        "    frames = np.array(frames)  # (VIDEO_LENGTH, SIZE, SIZE, CHANNEL)\n",
        "    frames = np.transpose(frames, (0, 3, 1, 2))  # (VIDEO_LENGTH, CHANNEL, SIZE, SIZE)\n",
        "    return torch.tensor(frames, dtype=torch.float32)\n",
        "\n",
        "def process_images(image_paths, size=(256, 256)):\n",
        "    \"\"\"\n",
        "    Process a list of image files into a tensor of shape (NUM_IMAGES, CHANNEL, SIZE, SIZE).\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    for img_path in sorted(image_paths):\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.resize(img, size)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        images.append(img)\n",
        "\n",
        "    # Convert to tensor (NUM_IMAGES, CHANNEL, SIZE, SIZE)\n",
        "    images = np.array(images)  # (NUM_IMAGES, SIZE, SIZE, CHANNEL)\n",
        "    images = np.transpose(images, (0, 3, 1, 2))  # (NUM_IMAGES, CHANNEL, SIZE, SIZE)\n",
        "    return torch.tensor(images, dtype=torch.float32)\n",
        "\n",
        "def create_tensors(data_dir, size=(256, 256)):\n",
        "    \"\"\"\n",
        "    Create tensors for all videos and images in the given directory.\n",
        "    \"\"\"\n",
        "    video_tensors = []\n",
        "    image_tensors = []\n",
        "\n",
        "    for folder in sorted(os.listdir(data_dir)):\n",
        "        folder_path = os.path.join(data_dir, folder)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "\n",
        "        # Process video\n",
        "        video_path = glob(os.path.join(folder_path, \"*.gif\"))[0]\n",
        "        video_tensor = process_video(video_path, size=size)\n",
        "        video_tensors.append(video_tensor)\n",
        "\n",
        "        # Process images\n",
        "        image_paths = glob(os.path.join(folder_path, \"*.jpg\"))\n",
        "        image_tensor = process_images(image_paths, size=size)\n",
        "        image_tensors.append(image_tensor)\n",
        "\n",
        "    # Stack tensors into final shapes\n",
        "    video_tensors = torch.stack(video_tensors)  # (NUMBER_OF_VIDEOS, VIDEO_LENGTH, CHANNEL, SIZE, SIZE)\n",
        "    image_tensors = torch.stack(image_tensors)  # (NUMBER_OF_VIDEOS, VIDEO_LENGTH, CHANNEL, SIZE, SIZE)\n",
        "\n",
        "    return video_tensors, image_tensors\n",
        "\n",
        "# Example usage\n",
        "data_dir = \"frms\"  # Replace with the path to your data directory\n",
        "videos1, videos2 = create_tensors(data_dir)\n",
        "print(\"Videos Tensor Shape:\", videos1.shape)\n",
        "print(\"Images Tensor Shape:\", videos2.shape)\n",
        "\n",
        "\n",
        "# adjust these parameterz\n",
        "NUMBER_OF_VIDEOS = 6\n",
        "VIDEO_LENGTH = 32\n",
        "CHANNEL = 3\n",
        "SIZE = 256\n",
        "\n",
        "device = torch.device(\"cpu\") # or \"cuda\"\n",
        "\n",
        "\n",
        "#uncomment the metrics you want to compute\n",
        "import json\n",
        "result = {}\n",
        "only_final = False\n",
        "# only_final = True\n",
        "# result['fvd'] = calculate_fvd(videos1, videos2, device, method='styleganv', only_final=only_final)\n",
        "result['fvd'] = calculate_fvd(videos1, videos2, device, method='videogpt', only_final=only_final)\n",
        "# result['ssim'] = calculate_ssim(videos1, videos2, only_final=only_final)\n",
        "# result['psnr'] = calculate_psnr(videos1, videos2, only_final=only_final)\n",
        "# result['lpips'] = calculate_lpips(videos1, videos2, device, only_final=only_final)\n",
        "print(json.dumps(result, indent=4))"
      ],
      "metadata": {
        "id": "-QkYzxc93QNh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}